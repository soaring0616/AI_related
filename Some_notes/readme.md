### Weaviate_v4_update.ipynb
æ”¹ç‰ˆæ¸¬è©¦ç´€éŒ„

### Some_Mathematical_Intuitions_and_Definitions_in_Calculus__Probability_and_Linear_algebra.pdf
ä¸€äº›æ©Ÿå™¨å­¸ç¿’(æˆ–æ˜¯æ‡‰ç”¨æ•¸å­¸)ä¸­ï¼Œå°æ–¼å¾®ç©åˆ†ã€ç·šæ€§ä»£æ•¸ã€å’Œæ©Ÿç‡é‡è¦çš„notion
* å¾®ç©åˆ†ï¼š
  * é€£çºŒ: èªªæ˜ function æ‰€è™•ä¹‹ domain çš„å®Œå–„
  * æ¥µé™: èªªæ˜ the availablity of differentiation
  * æ”¶æ–‚: èªªæ˜ the availablity of integration
* ç·šæ€§ä»£æ•¸ï¼š
  * å‘é‡ç©ºé–“: èªªæ˜ç·šæ€§è½‰æ›æ‰€åŸ·è¡Œçš„ domain éœ€ä¿æŒä¸€äº›ç®—æ•¸çš„åŸºæœ¬æ€§è³ª(åˆ†é…å¾‹ã€äº¤æ›å¾‹ etc)
  * ç·šæ€§è½‰æ›: äº†è§£ä»€éº¼ç¨±ç‚ºç·šæ€§è½‰æ›(æ•¸ä¹˜ã€ç–ŠåŠ )
  * çŸ©é™£èˆ‡ç·šæ€§è½‰æ›çš„é—œä¿‚: éœ€è¦åŸºåº•ä¾†å°‡æŠ½è±¡åŒ–çš„ç·šæ€§è½‰æ›å»ºæ§‹å‡ºæ•¸å€¼è¡¨ç¾çš„çŸ©é™£
* æ©Ÿç‡:
  * measure & measurable
  * probability as a measure function
  * random variable as a mapping function 

### A_study_of_attention_mechanics_in_the_industry_of_artificial_intelligence_202410.pdf

* æœ€å—ç›Šçš„æ˜¯å¹¾å€‹é»ï¼š
1. Attention çš„qå’Œkçš„å…§ç©å…¶å¯¦æ˜¯å‚³çµ±çµ±è¨ˆå­¸çš„æ±è¥¿ï¼Œå†æ ¹æœ¬çš„å«ç¾©å…¶å¯¦æ˜¯æœŸæœ›å€¼
2. å¾kernelæ¡ç´äº†Gaussian filterï¼Œsoftmaxè‡ªç„¶è€Œç„¶å°±å‘¼ä¹‹æ¬²å‡º
3. å°±æ˜¯å°æ–¼masked, self-attention, multi-head attention ä»–å€‘çš„è¨ˆç®—éç¨‹æ¯”è¼ƒç†Ÿæ‚‰ä¸€é»ï¼ˆè£é¢ä¹Ÿæœ‰context vectorè¨ˆç®—ï¼‰
4.  Additive Attention æ˜¯åœ¨è¨ˆç®—æˆæœ¬çš„è€ƒé‡ä¸‹ï¼Œå¯ä½œç‚ºæ›¿ä»£çš„æ–¹æ¡ˆï¼›

* æ¥ä¸‹ä¾†æ˜¯åœ¨æ‰¾ attention æ™‚æ‰¾åˆ°çš„è³‡æºï¼Œé›–ç„¶ä¸æ–¹ä¾¿æ”¾åœ¨æ­£å¼æ–‡ä»¶ï¼Œä½†é‚„æ˜¯å¯ä»¥ç”¨ä¾† presentation çš„ï¼š
  * ä»¥ä¸‹æ˜¯ä¸»è¦åƒè€ƒçš„æ–‡ç»
    * https://classic.d2l.ai/chapter_attention-mechanisms/nadaraya-watson.html
    * https://en.wikipedia.org/wiki/Attention_(machine_learning)
    * https://arxiv.org/abs/2204.13154
    * https://arxiv.org/pdf/1409.0473

  * æ¥ä¸‹ä¾†æ˜¯æœ‰åƒè€ƒåƒ¹å€¼çš„è³‡æº
    * https://www.kaggle.com/code/lianghsunhuang/attention-mechanism
    * https://lilianweng.github.io/posts/2018-06-24-attention/#summary
    * https://en.wikipedia.org/wiki/Neural_machine_translation
    * https://socialsci.libretexts.org/Bookshelves/Psychology/Cognitive_Psychology/Cognitive_Psychology_(Andrade_and_Walker)/11%3A_Attention/11.02%3A_History_of_Attention
    * https://distill.pub/2016/augmented-rnns/#neural-turing-machines
    * https://jaketae.github.io/study/seq2seq-attention/

  * æœ€å¾Œå°±æ˜¯æ„Ÿè¬ NotebookLM å’Œ Chatgptï¼›
Chatgpt æ¯”è¼ƒå®¹æ˜“ç™¼æ•£ï¼Œä½† NotebookLM çš„å¥½è™•ä¹‹ä¸€å°±æ˜¯ã€Œåƒ…é™æ–¼ã€ä½¿ç”¨è€…æ‰€æä¾›çš„è³‡æºï¼›
æ•…æ­¤ï¼Œä½¿ç”¨é †åºæ˜¯å…ˆé€é NotebookLM ï¼Œå†è®“ Chatgpt ä¾†ç™¼æ®ã€‚

Ummï¼Œæˆ–è¨±é€™ä¹Ÿæ˜¯ä¹‹å¾Œ AI ä½¿ç”¨è€…æ‰€éœ€è¦åŸ¹é¤Šçš„ç´ é¤Šå§ ğŸ˜
